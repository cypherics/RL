{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cypherics/RL/blob/3.2/assignment_3/assignment3_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rr1d-kHohxGL",
    "outputId": "1b585f80-7509-4e96-d4ef-11c77d1e87cd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gym[atari,accept-rom-license]==0.25.2\n",
    "import sys, os\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGCa_JQeiy1F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "from gym.spaces import Box\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        transform = torchvision.transforms.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(self.shape),\n",
    "                                                     torchvision.transforms.Normalize(0, 255)])\n",
    "        return transforms(observation).squeeze(0)\n",
    "\n",
    "\n",
    "class ExperienceReplayMemory(object):\n",
    "    def __init__(self, capacity, n_step=1, gamma=0.95):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _get_n_step_info(self):\n",
    "        r = 0\n",
    "        for idx in range(self.n_step):\n",
    "            r += self.gamma ** idx * self.n_step_buffer[idx][2]\n",
    "        return self.n_step_buffer[0][0], self.n_step_buffer[0][1], r, self.n_step_buffer[-1][3], self.n_step_buffer[-1][4]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def store(self, state, next_state, action, reward, done):\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return\n",
    "        state, action, reward, next_state, done = self._get_n_step_info()\n",
    "        self.memory.append((state, next_state, action, reward, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # TODO: uniformly sample batches of Tensors for: state, next_state, action, reward, done\n",
    "        # ...\n",
    "\n",
    "\n",
    "        # uniformly get batch with batch_size\n",
    "        sampled_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "\n",
    "        # save to arrays\n",
    "        for (curr_state, next_state, action, reward, done) in sampled_batch:\n",
    "            states.append(curr_state)\n",
    "            next_states.append(next_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "\n",
    "        return  torch.tensor(np.array(states)), \\\n",
    "                torch.tensor(np.array(next_states)), \\\n",
    "                torch.tensor(np.array(actions)), \\\n",
    "                torch.tensor(np.array(rewards)), \\\n",
    "                torch.tensor(np.array(dones))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOCiUgfBjJYc",
    "outputId": "5a08504e-a71b-4a18-f9b5-298d174eda3e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from gym.wrappers import FrameStack\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "env_rendering = False    # Set to False while training your model on Colab\n",
    "testing_mode = True\n",
    "test_model_directory = os.path.join(os.getcwd(), \"weights.pth\")\n",
    "\n",
    "# Create and preprocess the Space Invaders environment\n",
    "if env_rendering:\n",
    "    env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space=False, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space=False)\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "image_stack, h, w = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "print('Number of stacked frames: ', image_stack)\n",
    "print('Resized observation space dimensionality: ', h, w)\n",
    "print('Number of available actions by the agent: ', num_actions)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 61\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.backends.cudnn.enabled:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Hyperparameters (to be modified)\n",
    "batch_size = 32\n",
    "alpha = 0.00025\n",
    "gamma = 0.95\n",
    "eps, eps_decay, min_eps = 1.0, 0.999, 0.05\n",
    "n_step = 3\n",
    "buffer = ExperienceReplayMemory(20000, n_step=n_step, gamma=gamma)\n",
    "burn_in_phase = 20000\n",
    "sync_target = 30000\n",
    "max_train_frames = 10000\n",
    "max_train_episodes = 100000\n",
    "max_test_episodes = 1000\n",
    "curr_step = 0\n",
    "learning_rate = 0.001\n",
    "\n",
    "run_as_ddqn = False\n",
    "use_huber_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNoUgNjujqRX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def convert(x):\n",
    "    return torch.tensor(x.__array__()).float()\n",
    "\n",
    "\n",
    "class DeepQNet(torch.nn.Module):\n",
    "    def __init__(self, h, w, image_stack, num_actions):\n",
    "        super(DeepQNet, self).__init__()\n",
    "        # TODO: create a convolutional neural network\n",
    "        # ...\n",
    "\n",
    "        # self.conv = torch.nn.Sequential(\n",
    "        #     torch.nn.Conv2d(4, 6, 5),\n",
    "        #     torch.nn.ReLU(),\n",
    "        #     torch.nn.MaxPool2d(2,2),\n",
    "        #     torch.nn.Conv2d(6,16,5),\n",
    "        #     torch.nn.ReLU(),\n",
    "        #     torch.nn.MaxPool2d(2,2)\n",
    "        # )\n",
    "\n",
    "        # self.out_size = self.get_out(h,w)\n",
    "        # self.fully_connected_layers = torch.nn.Sequential(\n",
    "        #     torch.nn.Linear(self.out_size, 128),\n",
    "        #     torch.nn.Linear(128, 64),\n",
    "        #     torch.nn.Linear(64, num_actions)\n",
    "        # )\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=(8, 8), stride=4),\n",
    "            nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(4, 4), stride=2),\n",
    "            nn.ReLU())\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=1),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=7*7*64, out_features=256),\n",
    "            nn.ReLU())\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=num_actions)\n",
    "\n",
    "    def get_out(self, h,w):\n",
    "        out = self.conv(torch.zeros(1, 4, h, w))\n",
    "        return int(np.prod(out.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: forward pass from the neural network\n",
    "        # ...\n",
    "        out1 = self.conv1(x)\n",
    "        out2 = self.conv2(out1)        \n",
    "        out3 = self.conv3(out2)\n",
    "        out4 = self.fc1(out3.view(-1, 7*7*64))        \n",
    "        out = self.fc2(out4)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: create an online and target DQN (Hint: Use copy.deepcopy() and requires_grad utilities!)\n",
    "# ...\n",
    "online_dqn = DeepQNet(h,w,image_stack, num_actions)\n",
    "target_dqn = copy.deepcopy(online_dqn)\n",
    "online_dqn.to(device)\n",
    "target_dqn.to(device)\n",
    "\n",
    "for param in target_dqn.parameters():\n",
    "    param.requires_grad = False\n",
    "# TODO: create the appropriate MSE criterion and Adam optimizer\n",
    "# ...\n",
    "optimizer = torch.optim.Adam(online_dqn.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.HuberLoss(reduction=\"mean\", delta=1.0) if use_huber_loss else torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO21LQJ6j0WC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy(state, is_training):\n",
    "    global eps\n",
    "    state = convert(state).unsqueeze(0).to(device)\n",
    "    # state = (1, batch, h, w)\n",
    "\n",
    "    #TODO: Implement an epsilon-greedy policy\n",
    "    #...\n",
    "    # state_c = torch.from_numpy(state).float()/255.0\n",
    "    # state = Variable(state).cuda()\n",
    "    \n",
    "    # online_dqn.eval()\n",
    "    # estimate = online_dqn.forward(state).max(dim=1)\n",
    "    \n",
    "    # # with epsilon prob to choose random action else choose argmax Q estimate action\n",
    "    # if random.random() < self.epsilon:\n",
    "    #     return random.randint(0, self.action_number-1)\n",
    "    # else:\n",
    "    #     return estimate[1].data[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if is_training:\n",
    "            p = online_dqn(state)\n",
    "            # P = (1, 6)\n",
    "            if np.random.rand() < eps:\n",
    "                a = random_action()\n",
    "            else:\n",
    "                a = torch.argmax(p, dim=1).tolist()\n",
    "\n",
    "        else:\n",
    "            p = online_dqn(state)\n",
    "            a = torch.argmax(p, dim=1).tolist()\n",
    "\n",
    "    return convert(np.array(a)).to(device)\n",
    "\n",
    "def random_action():\n",
    "    return np.random.randint(0, num_actions)\n",
    "\n",
    "def compute_loss(state, action, reward, next_state, done):\n",
    "    state = convert(state).to(device)\n",
    "    next_state = convert(next_state).to(device)\n",
    "    action = action.view(-1, 1).to(device)\n",
    "    reward = reward.view(-1, 1).to(device)\n",
    "    done = done.view(-1, 1).to(device)\n",
    "\n",
    "    # TODO: Compute the DQN (or DDQN) loss based on the criterion\n",
    "    # ...\n",
    "\n",
    "    # mse loss\n",
    "    online_dqn.eval()\n",
    "    target_dqn.eval()\n",
    "    with torch.no_grad():\n",
    "        if run_as_ddqn:\n",
    "            # action_new = online_dqn.forward(next_state).max(dim=1)[1].cpu().data.view(-1, 1).to(device)\n",
    "            action_new = torch.argmax(online_dqn.forward(next_state), dim=1).view(-1, 1)\n",
    "            y = reward + torch.mul((torch.gather(target_dqn.forward(next_state), dim=1, index=action_new) * (~done)), (gamma ** n_step))\n",
    "        else:\n",
    "            y = reward + torch.mul((target_dqn.forward(next_state).max(dim=1)[0]).view(-1, 1) * (~done), (gamma ** n_step))\n",
    "\n",
    "    online_dqn.train()\n",
    "    Q = (torch.gather(online_dqn.forward(state), dim=1, index=action))\n",
    "\n",
    "    loss = criterion(input=Q.float(), target=y.float().detach())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_episode(curr_step, buffer, is_training):\n",
    "    global eps\n",
    "    global target_dqn\n",
    "    global online_dqn\n",
    "    episode_reward, episode_loss = 0, 0.\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(max_train_frames):\n",
    "        action = policy(state, is_training)\n",
    "        curr_step += 1\n",
    "        next_state, reward, done, _ = env.step(int(action.item()))\n",
    "        episode_reward += reward\n",
    "\n",
    "        if is_training:\n",
    "            buffer.store(state, next_state, int(action.item()), reward, done)\n",
    "\n",
    "            if curr_step > burn_in_phase:\n",
    "                state_batch, next_state_batch, action_batch, reward_batch, done_batch = buffer.sample(batch_size)\n",
    "\n",
    "                if curr_step % sync_target == 0:\n",
    "                    # TODO: Periodically update your target_dqn at each sync_target frames\n",
    "                    # ...\n",
    "                     target_dqn.load_state_dict(online_dqn.state_dict())\n",
    "                     for param in target_dqn.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "                loss = compute_loss(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                episode_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                episode_loss += compute_loss(state, action.type(torch.int64), torch.tensor(np.array(reward)), next_state, torch.tensor(np.array(done))).item()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "\n",
    "    return dict(reward=episode_reward, loss=episode_loss / t), curr_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEGSGYsQj8Wh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_metrics(metrics, episode):\n",
    "    for k, v in episode.items():\n",
    "        metrics[k].append(v)\n",
    "\n",
    "\n",
    "def print_metrics(it, metrics, is_training, window=100):\n",
    "    reward_mean = np.mean(metrics['reward'][-window:])\n",
    "    loss_mean = np.mean(metrics['loss'][-window:])\n",
    "    mode = \"train\" if is_training else \"test\"\n",
    "    print(f\"Episode {it:4d} | {mode:5s} | reward {reward_mean:5.5f} | loss {loss_mean:5.5f}\")\n",
    "\n",
    "\n",
    "def save_checkpoint(curr_step, eps, train_metrics):\n",
    "    save_dict = {'curr_step': curr_step, \n",
    "                 'train_metrics': train_metrics, \n",
    "                 'eps': eps,\n",
    "                 'online_dqn': online_dqn.state_dict(), \n",
    "                 'target_dqn': target_dqn.state_dict()}\n",
    "    torch.save(save_dict, test_model_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy7C4qfWkzXb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Plot your train_metrics and test_metrics\n",
    "# ...\n",
    "def plot_metrics(metrics, window=100):\n",
    "    reward = metrics['reward'][-window:]\n",
    "    loss = metrics['loss'][-window:]\n",
    "\n",
    "    reward = [r for idx, r in enumerate(metrics['reward']) if idx % 50 == 0]\n",
    "    loss = [r for idx, r in enumerate(metrics['loss']) if idx % 50 == 0]\n",
    "    epsiodes = np.arange(0, max_train_episodes, 50)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(epsiodes, reward)\n",
    "    ax2.plot(epsiodes, loss)\n",
    "\n",
    "    ax1.set_xlabel(\"episodes\")\n",
    "    ax2.set_xlabel(\"episodes\")\n",
    "    ax1.set_ylabel(\"reward\")\n",
    "    ax2.set_ylabel(\"loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_m(inp, name):\n",
    "    # load json module\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    # create json object from dictionary\n",
    "    json = json.dumps(inp)\n",
    "\n",
    "    # open file for writing, \"w\" \n",
    "    f = open(os.path.join(os.getcwd(), f\"{name}_metrics.json\"),\"w\")\n",
    "\n",
    "    # write json object to file\n",
    "    f.write(json)\n",
    "\n",
    "    # close file\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdhWcoW-kyuF",
    "outputId": "6a479932-0199-4797-d978-f4b623bba999",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics = None\n",
    "if testing_mode:\n",
    "    # TODO: Load your saved online_dqn model for evaluation\n",
    "    # ...\n",
    "    loaded_pth_file = torch.load(test_model_directory)\n",
    "    online_dqn = DeepQNet(h,w,image_stack,num_actions)\n",
    "    online_dqn.load_state_dict(loaded_pth_file['online_dqn'])\n",
    "    online_dqn.cuda()\n",
    "    test_metrics = dict(reward=[], loss=[])\n",
    "    for it in range(max_test_episodes):\n",
    "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=False)\n",
    "        update_metrics(test_metrics, episode_metrics)\n",
    "        print_metrics(it + 1, test_metrics, is_training=False)\n",
    "#         save_m(test_metrics, \"test\")\n",
    "\n",
    "    metrics = test_metrics\n",
    "else:\n",
    "    print(\"Training\")\n",
    "    train_metrics = dict(reward=[], loss=[])\n",
    "    for it in range(max_train_episodes):\n",
    "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=True)\n",
    "        update_metrics(train_metrics, episode_metrics)\n",
    "        if curr_step > burn_in_phase and eps > min_eps:\n",
    "            eps *= eps_decay\n",
    "        if it % 50 == 0:\n",
    "            print_metrics(it, train_metrics, is_training=True)\n",
    "            save_checkpoint(curr_step, eps, train_metrics)\n",
    "\n",
    "        # print(f\"episode: {it} done!\")\n",
    "#         save_m(train_metrics, \"train\")\n",
    "    metrics = train_metrics\n",
    "    # save_m(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQbMdor_dCnp"
   },
   "outputs": [],
   "source": [
    "  plot_metrics(metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
