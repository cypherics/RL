{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cypherics/RL/blob/3.2/assignment_3/assignment3_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rr1d-kHohxGL",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e49277e-ac70-457a-e60d-7c2c921ca594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari]==0.25.2 in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (0.7.5)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (0.4.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]==0.25.2) (5.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2.25.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (4.64.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (0.5.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]==0.25.2) (3.11.0)\n",
            "Requirement already satisfied: libtorrent in /usr/local/lib/python3.8/dist-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[atari,accept-rom-license]==0.25.2\n",
        "import sys, os\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "from gym.spaces import Box\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        transform = torchvision.transforms.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(self.shape),\n",
        "                                                     torchvision.transforms.Normalize(0, 255)])\n",
        "        return transforms(observation).squeeze(0)\n",
        "\n",
        "\n",
        "class ExperienceReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def store(self, state, next_state, action, reward, done):\n",
        "        state = state.__array__()\n",
        "        next_state = next_state.__array__()\n",
        "        self.memory.append((state, next_state, action, reward, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # TODO: uniformly sample batches of Tensors for: state, next_state, action, reward, done\n",
        "        # ...\n",
        "\n",
        "\n",
        "        # uniformly get batch with batch_size\n",
        "        sampled_batch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        states = []\n",
        "        next_states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "\n",
        "        # save to arrays\n",
        "        for (curr_state, next_state, action, reward, done) in sampled_batch:\n",
        "            states.append(curr_state)\n",
        "            next_states.append(next_state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "\n",
        "\n",
        "        return  torch.tensor(np.array(states)), \\\n",
        "                torch.tensor(np.array(next_states)), \\\n",
        "                torch.tensor(np.array(actions)), \\\n",
        "                torch.tensor(np.array(rewards)), \\\n",
        "                torch.tensor(np.array(dones))\n"
      ],
      "metadata": {
        "id": "IGCa_JQeiy1F",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "import numpy as np\n",
        "import copy\n",
        "from gym.wrappers import FrameStack\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "env_rendering = False    # Set to False while training your model on Colab\n",
        "testing_mode = False\n",
        "test_model_directory = '/content/sample_data/your_saved_model.pth'\n",
        "\n",
        "# Create and preprocess the Space Invaders environment\n",
        "if env_rendering:\n",
        "    env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space=False, render_mode=\"human\")\n",
        "else:\n",
        "    env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space=False)\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = FrameStack(env, num_stack=4)\n",
        "image_stack, h, w = env.observation_space.shape\n",
        "num_actions = env.action_space.n\n",
        "print('Number of stacked frames: ', image_stack)\n",
        "print('Resized observation space dimensionality: ', h, w)\n",
        "print('Number of available actions by the agent: ', num_actions)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "seed = 61\n",
        "env.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.backends.cudnn.enabled:\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Hyperparameters (to be modified)\n",
        "batch_size = 32\n",
        "alpha = 0.00025\n",
        "gamma = 0.95\n",
        "eps, eps_decay, min_eps = 1.0, 0.999, 0.05\n",
        "buffer = ExperienceReplayMemory(20000)\n",
        "burn_in_phase = 20000\n",
        "sync_target = 30000\n",
        "max_train_frames = 10000\n",
        "max_train_episodes = 3000# 100000\n",
        "max_test_episodes = 1\n",
        "curr_step = 0\n",
        "learning_rate = 0.001\n"
      ],
      "metadata": {
        "id": "LOCiUgfBjJYc",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dfb81f8-e020-4f90-fcab-5e582b61f80c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stacked frames:  4\n",
            "Resized observation space dimensionality:  84 84\n",
            "Number of available actions by the agent:  6\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def convert(x):\n",
        "    return torch.tensor(x.__array__()).float()\n",
        "\n",
        "\n",
        "class DeepQNet(torch.nn.Module):\n",
        "    def __init__(self, h, w, image_stack, num_actions):\n",
        "        super(DeepQNet, self).__init__()\n",
        "        # TODO: create a convolutional neural network\n",
        "        # ...\n",
        "\n",
        "        self.conv = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(4, 6, 5),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(2,2),\n",
        "            torch.nn.Conv2d(6,16,5),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(2,2)\n",
        "        )\n",
        "\n",
        "        self.out_size = self.get_out(h,w)\n",
        "        self.fully_connected_layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.out_size, 128),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.Linear(64, num_actions)\n",
        "        )\n",
        "\n",
        "    def get_out(self, h,w):\n",
        "        out = self.conv(torch.zeros(1, 4, h, w))\n",
        "        return int(np.prod(out.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: forward pass from the neural network\n",
        "        # ...\n",
        "        x_new = self.conv(x)\n",
        "        x_new = x_new.view(-1, self.out_size)\n",
        "        return self.fully_connected_layers(x_new)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: create an online and target DQN (Hint: Use copy.deepcopy() and requires_grad utilities!)\n",
        "# ...\n",
        "online_dqn = DeepQNet(h,w,image_stack, num_actions)\n",
        "target_dqn = copy.deepcopy(online_dqn)\n",
        "online_dqn.to(device)\n",
        "target_dqn.to(device)\n",
        "\n",
        "\n",
        "# TODO: create the appropriate MSE criterion and Adam optimizer\n",
        "# ...\n",
        "optimizer = torch.optim.Adam(online_dqn.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.MSELoss()\n"
      ],
      "metadata": {
        "id": "NNoUgNjujqRX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy(state, is_training):\n",
        "    global eps\n",
        "    state = convert(state).unsqueeze(0).to(device)\n",
        "\n",
        "    #TODO: Implement an epsilon-greedy policy\n",
        "    #...\n",
        "    with torch.no_grad():\n",
        "        if is_training:\n",
        "            p = online_dqn(state)\n",
        "            if np.random.rand() < eps:\n",
        "                a = random_action()\n",
        "            else:\n",
        "                a = torch.argmax(p).tolist()\n",
        "\n",
        "        else:\n",
        "            p = online_dqn(state)\n",
        "            a = torch.argmax(p).tolist()\n",
        "\n",
        "    return a\n",
        "\n",
        "def random_action():\n",
        "    return np.random.randint(0, num_actions)\n",
        "\n",
        "def compute_loss(state, action, reward, next_state, done):\n",
        "    state = convert(state).to(device)\n",
        "    next_state = convert(next_state).to(device)\n",
        "    action = action.view(batch_size, 1).to(device)\n",
        "    reward = reward.view(batch_size, 1).to(device)\n",
        "    done = done.view(batch_size, 1).to(device)\n",
        "\n",
        "    # TODO: Compute the DQN (or DDQN) loss based on the criterion\n",
        "    # ...\n",
        "\n",
        "    # mse loss\n",
        "    online_dqn.eval()\n",
        "    target_dqn.eval()\n",
        "\n",
        "    # action_new = online_dqn.forward(next_state).max(dim=1)[1].cpu().data.view(-1, 1).to(device)\n",
        "    action_new = torch.argmax(online_dqn.forward(next_state), dim=1).view(-1, 1)\n",
        "    target = target_dqn.forward(next_state)\n",
        "    y_target =  torch.gather(target, dim=1, index=action_new)\n",
        "    y = reward + torch.mul((y_target * done), gamma)\n",
        "\n",
        "\n",
        "    online_dqn.train()\n",
        "    Q = (torch.gather(online_dqn.forward(state), dim=1, index=action))\n",
        "\n",
        "    loss = criterion(input=Q.float(), target=y.float().detach())\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def run_episode(curr_step, buffer, is_training):\n",
        "    global eps\n",
        "    global target_dqn\n",
        "    episode_reward, episode_loss = 0, 0.\n",
        "    state = env.reset()\n",
        "    \n",
        "    for t in range(max_train_frames):\n",
        "        action = policy(state, is_training)\n",
        "        curr_step += 1\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if is_training:\n",
        "            buffer.store(state, next_state, action, reward, done)\n",
        "\n",
        "            if curr_step > burn_in_phase:\n",
        "                state_batch, next_state_batch, action_batch, reward_batch, done_batch = buffer.sample(batch_size)\n",
        "\n",
        "                if curr_step % sync_target == 0:\n",
        "                    # TODO: Periodically update your target_dqn at each sync_target frames\n",
        "                    # ...\n",
        "                    target_dqn = copy.deepcopy(online_dqn)\n",
        "\n",
        "                loss = compute_loss(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                episode_loss += loss.item()\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                episode_loss += compute_loss(state, action, reward, next_state, done).item()\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "\n",
        "    return dict(reward=episode_reward, loss=episode_loss / t), curr_step\n"
      ],
      "metadata": {
        "id": "RO21LQJ6j0WC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_metrics(metrics, episode):\n",
        "    for k, v in episode.items():\n",
        "        metrics[k].append(v)\n",
        "\n",
        "\n",
        "def print_metrics(it, metrics, is_training, window=100):\n",
        "    reward_mean = np.mean(metrics['reward'][-window:])\n",
        "    loss_mean = np.mean(metrics['loss'][-window:])\n",
        "    mode = \"train\" if is_training else \"test\"\n",
        "    print(f\"Episode {it:4d} | {mode:5s} | reward {reward_mean:5.5f} | loss {loss_mean:5.5f}\")\n",
        "\n",
        "\n",
        "def save_checkpoint(curr_step, eps, train_metrics):\n",
        "    save_dict = {'curr_step': curr_step, \n",
        "                 'train_metrics': train_metrics, \n",
        "                 'eps': eps,\n",
        "                 'online_dqn': online_dqn.state_dict(), \n",
        "                 'target_dqn': target_dqn.state_dict()}\n",
        "    torch.save(save_dict, test_model_directory)\n"
      ],
      "metadata": {
        "id": "PEGSGYsQj8Wh",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot your train_metrics and test_metrics\n",
        "# ...\n",
        "def plot_metrics(metrics, window=100):\n",
        "    reward = metrics['reward'][-window:]\n",
        "    loss = metrics['loss'][-window:]\n",
        "\n",
        "    reward = [r for idx, r in enumerate(metrics['reward']) if idx % 50 == 0]\n",
        "    loss = [r for idx, r in enumerate(metrics['loss']) if idx % 50 == 0]\n",
        "    epsiodes = np.arange(0, max_train_episodes, 50)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    ax1.plot(epsiodes, reward)\n",
        "    ax2.plot(epsiodes, loss)\n",
        "\n",
        "    ax1.set_xlabel(\"episodes\")\n",
        "    ax2.set_xlabel(\"episodes\")\n",
        "    ax1.set_ylabel(\"reward\")\n",
        "    ax2.set_ylabel(\"loss\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Uy7C4qfWkzXb",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if testing_mode:\n",
        "    # TODO: Load your saved online_dqn model for evaluation\n",
        "    # ...\n",
        "    loaded_pth_file = torch.load(test_model_directory)\n",
        "    online_dqn = DeepQNet(h,w,image_stack,num_actions)\n",
        "    online_dqn.load_state_dict(loaded_pth_file['online_dqn'])\n",
        "\n",
        "    test_metrics = dict(reward=[], loss=[])\n",
        "    for it in range(max_test_episodes):\n",
        "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=False)\n",
        "        update_metrics(test_metrics, episode_metrics)\n",
        "        print_metrics(it + 1, test_metrics, is_training=False)\n",
        "    plot_metrics(test_metrics)\n",
        "else:\n",
        "    train_metrics = dict(reward=[], loss=[])\n",
        "    for it in range(max_train_episodes):\n",
        "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=True)\n",
        "        update_metrics(train_metrics, episode_metrics)\n",
        "        if curr_step > burn_in_phase and eps > min_eps:\n",
        "            eps *= eps_decay\n",
        "        if it % 50 == 0:\n",
        "            print_metrics(it, train_metrics, is_training=True)\n",
        "            save_checkpoint(curr_step, eps, train_metrics)\n",
        "        # print(f\"episode: {it} done!\")\n"
      ],
      "metadata": {
        "id": "TdhWcoW-kyuF",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bdf0571b-b887-440f-c296-c82680109f72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode    0 | train | reward 30.00000 | loss 0.00000\n",
            "Episode   50 | train | reward 196.27451 | loss 0.00000\n",
            "Episode  100 | train | reward 174.20000 | loss 0.00000\n",
            "Episode  150 | train | reward 152.00000 | loss 5.81721\n",
            "Episode  200 | train | reward 135.60000 | loss 23.50106\n",
            "Episode  250 | train | reward 132.55000 | loss 27.66708\n",
            "Episode  300 | train | reward 146.25000 | loss 16.41404\n",
            "Episode  350 | train | reward 136.00000 | loss 11.98998\n",
            "Episode  400 | train | reward 126.50000 | loss 11.22136\n",
            "Episode  450 | train | reward 115.70000 | loss 11.14233\n",
            "Episode  500 | train | reward 138.25000 | loss 12.47163\n",
            "Episode  550 | train | reward 149.25000 | loss 13.53877\n",
            "Episode  600 | train | reward 132.45000 | loss 12.54132\n",
            "Episode  650 | train | reward 140.95000 | loss 12.35742\n",
            "Episode  700 | train | reward 151.35000 | loss 13.98215\n",
            "Episode  750 | train | reward 145.85000 | loss 14.51425\n",
            "Episode  800 | train | reward 146.45000 | loss 13.23431\n",
            "Episode  850 | train | reward 140.40000 | loss 13.80657\n",
            "Episode  900 | train | reward 134.50000 | loss 13.05378\n",
            "Episode  950 | train | reward 146.05000 | loss 11.39281\n",
            "Episode 1000 | train | reward 125.10000 | loss 10.97885\n",
            "Episode 1050 | train | reward 129.30000 | loss 9.86650\n",
            "Episode 1100 | train | reward 146.85000 | loss 9.51800\n",
            "Episode 1150 | train | reward 141.10000 | loss 9.80424\n",
            "Episode 1200 | train | reward 125.15000 | loss 9.05833\n",
            "Episode 1250 | train | reward 130.85000 | loss 8.22601\n",
            "Episode 1300 | train | reward 159.25000 | loss 10.09978\n",
            "Episode 1350 | train | reward 147.45000 | loss 11.28498\n",
            "Episode 1400 | train | reward 132.40000 | loss 10.72183\n",
            "Episode 1450 | train | reward 136.50000 | loss 9.62456\n",
            "Episode 1500 | train | reward 155.10000 | loss 9.92859\n",
            "Episode 1550 | train | reward 154.60000 | loss 11.48653\n",
            "Episode 1600 | train | reward 149.30000 | loss 12.33829\n",
            "Episode 1650 | train | reward 137.95000 | loss 11.72129\n",
            "Episode 1700 | train | reward 133.80000 | loss 9.96854\n",
            "Episode 1750 | train | reward 151.90000 | loss 9.59500\n",
            "Episode 1800 | train | reward 144.40000 | loss 8.97126\n",
            "Episode 1850 | train | reward 140.70000 | loss 8.61041\n",
            "Episode 1900 | train | reward 153.05000 | loss 9.30194\n",
            "Episode 1950 | train | reward 157.65000 | loss 10.98339\n",
            "Episode 2000 | train | reward 144.60000 | loss 10.70109\n",
            "Episode 2050 | train | reward 138.75000 | loss 10.07386\n",
            "Episode 2100 | train | reward 146.45000 | loss 10.77402\n",
            "Episode 2150 | train | reward 141.20000 | loss 10.66893\n",
            "Episode 2200 | train | reward 147.60000 | loss 11.42649\n",
            "Episode 2250 | train | reward 153.85000 | loss 11.15452\n",
            "Episode 2300 | train | reward 150.55000 | loss 11.63967\n",
            "Episode 2350 | train | reward 140.75000 | loss 12.36147\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b2c9c40389e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_train_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mepisode_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mupdate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurr_step\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mburn_in_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_eps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c4bf6ac26614>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(curr_step, buffer, is_training)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mcurr_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \"\"\"\n\u001b[1;32m    176\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         )\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-10d7de357c38>\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     47\u001b[0m         transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(self.shape),\n\u001b[1;32m     48\u001b[0m                                                      torchvision.transforms.Normalize(0, 255)])\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \"\"\"\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  plot_metrics(train_metrics)"
      ],
      "metadata": {
        "id": "RQbMdor_dCnp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}