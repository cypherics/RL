{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cypherics/RL/blob/3.2/assignment_3/assignment3_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rr1d-kHohxGL",
    "outputId": "1b585f80-7509-4e96-d4ef-11c77d1e87cd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gym[accept-rom-license,atari]==0.25.2 in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (6.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (2.2.0)\n",
      "Collecting ale-py~=0.7.5\n",
      "  Downloading ale_py-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]==0.25.2) (5.10.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2.25.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (7.1.2)\n",
      "Collecting AutoROM.accept-rom-license\n",
      "  Downloading AutoROM.accept-rom-license-0.5.4.tar.gz (12 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]==0.25.2) (3.11.0)\n",
      "Collecting libtorrent\n",
      "  Using cached libtorrent-2.0.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2.10)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.5.4-py3-none-any.whl size=441148 sha256=bff6424652b26091e7747fe0b693fa3466c8f99109edca98aa0ad1e457dc7fb7\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/60/90/db006a24f232de90641041430b5913a601345c9efc4cb883ea\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: libtorrent, AutoROM.accept-rom-license, autorom, ale-py\n",
      "Successfully installed AutoROM.accept-rom-license-0.5.4 ale-py-0.7.5 autorom-0.4.2 libtorrent-2.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari,accept-rom-license]==0.25.2\n",
    "import sys, os\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IGCa_JQeiy1F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "from gym.spaces import Box\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        transform = torchvision.transforms.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(self.shape),\n",
    "                                                     torchvision.transforms.Normalize(0, 255)])\n",
    "        return transforms(observation).squeeze(0)\n",
    "\n",
    "\n",
    "class ExperienceReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def store(self, state, next_state, action, reward, done):\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "        self.memory.append((state, next_state, action, reward, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # TODO: uniformly sample batches of Tensors for: state, next_state, action, reward, done\n",
    "        # ...\n",
    "\n",
    "\n",
    "        # uniformly get batch with batch_size\n",
    "        sampled_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "\n",
    "        # save to arrays\n",
    "        for (curr_state, next_state, action, reward, done) in sampled_batch:\n",
    "            states.append(curr_state)\n",
    "            next_states.append(next_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "\n",
    "        return  torch.tensor(np.array(states)), \\\n",
    "                torch.tensor(np.array(next_states)), \\\n",
    "                torch.tensor(np.array(actions)), \\\n",
    "                torch.tensor(np.array(rewards)), \\\n",
    "                torch.tensor(np.array(dones))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOCiUgfBjJYc",
    "outputId": "5a08504e-a71b-4a18-f9b5-298d174eda3e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stacked frames:  4\n",
      "Resized observation space dimensionality:  84 84\n",
      "Number of available actions by the agent:  6\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "from gym.wrappers import FrameStack\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "env_rendering = False    # Set to False while training your model on Colab\n",
    "testing_mode = True\n",
    "test_model_directory = '/content/sample_data/ddqn.pth'\n",
    "\n",
    "# Create and preprocess the Space Invaders environment\n",
    "if env_rendering:\n",
    "    env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space=False, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space=False)\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "image_stack, h, w = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "print('Number of stacked frames: ', image_stack)\n",
    "print('Resized observation space dimensionality: ', h, w)\n",
    "print('Number of available actions by the agent: ', num_actions)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 61\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.backends.cudnn.enabled:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Hyperparameters (to be modified)\n",
    "batch_size = 32\n",
    "alpha = 0.00025\n",
    "gamma = 0.95\n",
    "eps, eps_decay, min_eps = 1.0, 0.999, 0.05\n",
    "buffer = ExperienceReplayMemory(20000)\n",
    "burn_in_phase = 20000\n",
    "sync_target = 30000\n",
    "max_train_frames = 10000\n",
    "max_train_episodes = 100000\n",
    "max_test_episodes = 1000\n",
    "curr_step = 0\n",
    "learning_rate = 0.001\n",
    "\n",
    "run_as_ddqn = False\n",
    "use_huber_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NNoUgNjujqRX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def convert(x):\n",
    "    return torch.tensor(x.__array__()).float()\n",
    "\n",
    "\n",
    "class DeepQNet(torch.nn.Module):\n",
    "    def __init__(self, h, w, image_stack, num_actions):\n",
    "        super(DeepQNet, self).__init__()\n",
    "        # TODO: create a convolutional neural network\n",
    "        # ...\n",
    "\n",
    "        # self.conv = torch.nn.Sequential(\n",
    "        #     torch.nn.Conv2d(4, 6, 5),\n",
    "        #     torch.nn.ReLU(),\n",
    "        #     torch.nn.MaxPool2d(2,2),\n",
    "        #     torch.nn.Conv2d(6,16,5),\n",
    "        #     torch.nn.ReLU(),\n",
    "        #     torch.nn.MaxPool2d(2,2)\n",
    "        # )\n",
    "\n",
    "        # self.out_size = self.get_out(h,w)\n",
    "        # self.fully_connected_layers = torch.nn.Sequential(\n",
    "        #     torch.nn.Linear(self.out_size, 128),\n",
    "        #     torch.nn.Linear(128, 64),\n",
    "        #     torch.nn.Linear(64, num_actions)\n",
    "        # )\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=(8, 8), stride=4),\n",
    "            nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(4, 4), stride=2),\n",
    "            nn.ReLU())\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=1),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=7*7*64, out_features=256),\n",
    "            nn.ReLU())\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=num_actions)\n",
    "\n",
    "    def get_out(self, h,w):\n",
    "        out = self.conv(torch.zeros(1, 4, h, w))\n",
    "        return int(np.prod(out.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: forward pass from the neural network\n",
    "        # ...\n",
    "        out1 = self.conv1(x)\n",
    "        out2 = self.conv2(out1)        \n",
    "        out3 = self.conv3(out2)\n",
    "        out4 = self.fc1(out3.view(-1, 7*7*64))        \n",
    "        out = self.fc2(out4)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: create an online and target DQN (Hint: Use copy.deepcopy() and requires_grad utilities!)\n",
    "# ...\n",
    "online_dqn = DeepQNet(h,w,image_stack, num_actions)\n",
    "target_dqn = copy.deepcopy(online_dqn)\n",
    "online_dqn.to(device)\n",
    "target_dqn.to(device)\n",
    "\n",
    "for param in target_dqn.parameters():\n",
    "    param.requires_grad = False\n",
    "# TODO: create the appropriate MSE criterion and Adam optimizer\n",
    "# ...\n",
    "optimizer = torch.optim.Adam(online_dqn.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss() if use_huber_loss else torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RO21LQJ6j0WC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy(state, is_training):\n",
    "    global eps\n",
    "    state = convert(state).unsqueeze(0).to(device)\n",
    "    # state = (1, batch, h, w)\n",
    "\n",
    "    #TODO: Implement an epsilon-greedy policy\n",
    "    #...\n",
    "    # state_c = torch.from_numpy(state).float()/255.0\n",
    "    # state = Variable(state).cuda()\n",
    "    \n",
    "    # online_dqn.eval()\n",
    "    # estimate = online_dqn.forward(state).max(dim=1)\n",
    "    \n",
    "    # # with epsilon prob to choose random action else choose argmax Q estimate action\n",
    "    # if random.random() < self.epsilon:\n",
    "    #     return random.randint(0, self.action_number-1)\n",
    "    # else:\n",
    "    #     return estimate[1].data[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if is_training:\n",
    "            p = online_dqn(state)\n",
    "            # P = (1, 6)\n",
    "            if np.random.rand() < eps:\n",
    "                a = random_action()\n",
    "            else:\n",
    "                a = torch.argmax(p, dim=1).tolist()\n",
    "\n",
    "        else:\n",
    "            p = online_dqn(state)\n",
    "            a = torch.argmax(p, dim=1).tolist()\n",
    "\n",
    "    return convert(np.array(a)).to(device)\n",
    "\n",
    "def random_action():\n",
    "    return np.random.randint(0, num_actions)\n",
    "\n",
    "def compute_loss(state, action, reward, next_state, done):\n",
    "    state = convert(state).to(device)\n",
    "    next_state = convert(next_state).to(device)\n",
    "    action = action.view(-1, 1).to(device)\n",
    "    reward = reward.view(-1, 1).to(device)\n",
    "    done = done.view(-1, 1).to(device)\n",
    "\n",
    "    # TODO: Compute the DQN (or DDQN) loss based on the criterion\n",
    "    # ...\n",
    "\n",
    "    # mse loss\n",
    "    online_dqn.eval()\n",
    "    target_dqn.eval()\n",
    "    \n",
    "    if use_dqn:\n",
    "        # action_new = online_dqn.forward(next_state).max(dim=1)[1].cpu().data.view(-1, 1).to(device)\n",
    "        action_new = torch.argmax(online_dqn.forward(next_state), dim=1).view(-1, 1)\n",
    "        y = reward + torch.mul((torch.gather(target_dqn.forward(next_state), dim=1, index=action_new) * (~done)), gamma)\n",
    "    else:\n",
    "        y = reward + torch.mul((target_dqn.forward(next_state).max(dim=1)[0]).view(-1, 1), gamma)\n",
    "\n",
    "    online_dqn.train()\n",
    "    Q = (torch.gather(online_dqn.forward(state), dim=1, index=action))\n",
    "\n",
    "    loss = criterion(input=Q.float(), target=y.float().detach())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_episode(curr_step, buffer, is_training):\n",
    "    global eps\n",
    "    global target_dqn\n",
    "    global online_dqn\n",
    "    episode_reward, episode_loss = 0, 0.\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(max_train_frames):\n",
    "        action = policy(state, is_training)\n",
    "        curr_step += 1\n",
    "        next_state, reward, done, _ = env.step(int(action.item()))\n",
    "        episode_reward += reward\n",
    "\n",
    "        if is_training:\n",
    "            buffer.store(state, next_state, int(action.item()), reward, done)\n",
    "\n",
    "            if curr_step > burn_in_phase:\n",
    "                state_batch, next_state_batch, action_batch, reward_batch, done_batch = buffer.sample(batch_size)\n",
    "\n",
    "                if curr_step % sync_target == 0:\n",
    "                    # TODO: Periodically update your target_dqn at each sync_target frames\n",
    "                    # ...\n",
    "                     target_dqn.load_state_dict(online_dqn.state_dict())\n",
    "                     for param in target_dqn.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "                loss = compute_loss(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                episode_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                episode_loss += compute_loss(state, action.type(torch.int64), torch.tensor(np.array(reward)), next_state, torch.tensor(np.array(done))).item()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "\n",
    "    return dict(reward=episode_reward, loss=episode_loss / t), curr_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PEGSGYsQj8Wh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_metrics(metrics, episode):\n",
    "    for k, v in episode.items():\n",
    "        metrics[k].append(v)\n",
    "\n",
    "\n",
    "def print_metrics(it, metrics, is_training, window=100):\n",
    "    reward_mean = np.mean(metrics['reward'][-window:])\n",
    "    loss_mean = np.mean(metrics['loss'][-window:])\n",
    "    mode = \"train\" if is_training else \"test\"\n",
    "    print(f\"Episode {it:4d} | {mode:5s} | reward {reward_mean:5.5f} | loss {loss_mean:5.5f}\")\n",
    "\n",
    "\n",
    "def save_checkpoint(curr_step, eps, train_metrics):\n",
    "    save_dict = {'curr_step': curr_step, \n",
    "                 'train_metrics': train_metrics, \n",
    "                 'eps': eps,\n",
    "                 'online_dqn': online_dqn.state_dict(), \n",
    "                 'target_dqn': target_dqn.state_dict()}\n",
    "    torch.save(save_dict, test_model_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Uy7C4qfWkzXb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Plot your train_metrics and test_metrics\n",
    "# ...\n",
    "def plot_metrics(metrics, window=100):\n",
    "    reward = metrics['reward'][-window:]\n",
    "    loss = metrics['loss'][-window:]\n",
    "\n",
    "    reward = [r for idx, r in enumerate(metrics['reward']) if idx % 50 == 0]\n",
    "    loss = [r for idx, r in enumerate(metrics['loss']) if idx % 50 == 0]\n",
    "    epsiodes = np.arange(0, max_train_episodes, 50)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(epsiodes, reward)\n",
    "    ax2.plot(epsiodes, loss)\n",
    "\n",
    "    ax1.set_xlabel(\"episodes\")\n",
    "    ax2.set_xlabel(\"episodes\")\n",
    "    ax1.set_ylabel(\"reward\")\n",
    "    ax2.set_ylabel(\"loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_m(inp):\n",
    "    # load json module\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    # create json object from dictionary\n",
    "    json = json.dumps(inp)\n",
    "\n",
    "    # open file for writing, \"w\" \n",
    "    f = open(os.path.join(r\"/content/sample_data\", \"test_metrics.json\"),\"w\")\n",
    "\n",
    "    # write json object to file\n",
    "    f.write(json)\n",
    "\n",
    "    # close file\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdhWcoW-kyuF",
    "outputId": "6a479932-0199-4797-d978-f4b623bba999",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    1 | test  | reward 385.00000 | loss 1233.89625\n",
      "Episode    2 | test  | reward 307.50000 | loss 1201.24405\n",
      "Episode    3 | test  | reward 345.00000 | loss 1163.71614\n",
      "Episode    4 | test  | reward 316.25000 | loss 1258.17522\n",
      "Episode    5 | test  | reward 366.00000 | loss 1208.56416\n",
      "Episode    6 | test  | reward 370.83333 | loss 1141.37581\n",
      "Episode    7 | test  | reward 366.42857 | loss 1140.19656\n",
      "Episode    8 | test  | reward 360.00000 | loss 1175.75757\n",
      "Episode    9 | test  | reward 388.88889 | loss 1164.86644\n",
      "Episode   10 | test  | reward 381.50000 | loss 1164.59721\n",
      "Episode   11 | test  | reward 372.72727 | loss 1158.78119\n",
      "Episode   12 | test  | reward 369.16667 | loss 1179.69075\n",
      "Episode   13 | test  | reward 405.00000 | loss 1158.36561\n",
      "Episode   14 | test  | reward 405.35714 | loss 1148.39680\n",
      "Episode   15 | test  | reward 395.66667 | loss 1148.64304\n",
      "Episode   16 | test  | reward 404.06250 | loss 1145.31233\n",
      "Episode   17 | test  | reward 403.52941 | loss 1151.91048\n",
      "Episode   18 | test  | reward 418.33333 | loss 1143.54808\n",
      "Episode   19 | test  | reward 408.15789 | loss 1122.57234\n",
      "Episode   20 | test  | reward 422.00000 | loss 1115.47888\n",
      "Episode   21 | test  | reward 416.90476 | loss 1121.83779\n",
      "Episode   22 | test  | reward 409.54545 | loss 1109.89658\n",
      "Episode   23 | test  | reward 415.43478 | loss 1136.65385\n",
      "Episode   24 | test  | reward 426.04167 | loss 1130.11287\n",
      "Episode   25 | test  | reward 421.00000 | loss 1126.44598\n",
      "Episode   26 | test  | reward 413.65385 | loss 1127.05161\n",
      "Episode   27 | test  | reward 409.07407 | loss 1127.61759\n",
      "Episode   28 | test  | reward 407.85714 | loss 1132.39150\n",
      "Episode   29 | test  | reward 422.58621 | loss 1125.15188\n",
      "Episode   30 | test  | reward 416.00000 | loss 1117.24889\n",
      "Episode   31 | test  | reward 417.41935 | loss 1121.55898\n",
      "Episode   32 | test  | reward 415.00000 | loss 1124.41668\n",
      "Episode   33 | test  | reward 409.39394 | loss 1126.85612\n",
      "Episode   34 | test  | reward 408.97059 | loss 1133.43574\n",
      "Episode   35 | test  | reward 402.28571 | loss 1140.25445\n",
      "Episode   36 | test  | reward 400.69444 | loss 1139.30266\n",
      "Episode   37 | test  | reward 401.89189 | loss 1134.63249\n",
      "Episode   38 | test  | reward 397.36842 | loss 1135.86616\n",
      "Episode   39 | test  | reward 396.15385 | loss 1135.05601\n",
      "Episode   40 | test  | reward 399.37500 | loss 1130.56611\n",
      "Episode   41 | test  | reward 398.65854 | loss 1129.07625\n",
      "Episode   42 | test  | reward 396.66667 | loss 1125.17846\n",
      "Episode   43 | test  | reward 395.11628 | loss 1127.86270\n",
      "Episode   44 | test  | reward 396.02273 | loss 1126.88291\n",
      "Episode   45 | test  | reward 401.22222 | loss 1126.72494\n",
      "Episode   46 | test  | reward 401.63043 | loss 1124.48308\n",
      "Episode   47 | test  | reward 400.85106 | loss 1121.90646\n",
      "Episode   48 | test  | reward 398.54167 | loss 1117.79902\n",
      "Episode   49 | test  | reward 401.32653 | loss 1128.73489\n",
      "Episode   50 | test  | reward 405.20000 | loss 1128.60994\n",
      "Episode   51 | test  | reward 409.11765 | loss 1127.71416\n",
      "Episode   52 | test  | reward 407.59615 | loss 1129.66014\n",
      "Episode   53 | test  | reward 407.54717 | loss 1129.14224\n",
      "Episode   54 | test  | reward 410.46296 | loss 1124.03885\n",
      "Episode   55 | test  | reward 414.54545 | loss 1125.86260\n",
      "Episode   56 | test  | reward 414.10714 | loss 1129.00314\n",
      "Episode   57 | test  | reward 415.08772 | loss 1127.25066\n",
      "Episode   58 | test  | reward 412.41379 | loss 1128.88962\n",
      "Episode   59 | test  | reward 412.54237 | loss 1126.70847\n",
      "Episode   60 | test  | reward 410.33333 | loss 1123.34944\n",
      "Episode   61 | test  | reward 410.98361 | loss 1124.05258\n",
      "Episode   62 | test  | reward 408.95161 | loss 1123.13517\n",
      "Episode   63 | test  | reward 408.25397 | loss 1123.02202\n",
      "Episode   64 | test  | reward 412.65625 | loss 1126.70038\n",
      "Episode   65 | test  | reward 416.38462 | loss 1125.43407\n",
      "Episode   66 | test  | reward 415.15152 | loss 1128.26207\n",
      "Episode   67 | test  | reward 414.10448 | loss 1131.76516\n",
      "Episode   68 | test  | reward 416.02941 | loss 1134.66906\n",
      "Episode   69 | test  | reward 414.13043 | loss 1135.55319\n",
      "Episode   70 | test  | reward 411.92857 | loss 1137.40389\n",
      "Episode   71 | test  | reward 410.21127 | loss 1137.10382\n",
      "Episode   72 | test  | reward 410.97222 | loss 1136.60793\n",
      "Episode   73 | test  | reward 411.30137 | loss 1134.79093\n",
      "Episode   74 | test  | reward 408.37838 | loss 1129.94505\n",
      "Episode   75 | test  | reward 406.00000 | loss 1131.43436\n",
      "Episode   76 | test  | reward 411.11842 | loss 1132.34279\n",
      "Episode   77 | test  | reward 409.93506 | loss 1134.14037\n",
      "Episode   78 | test  | reward 409.10256 | loss 1133.19083\n",
      "Episode   79 | test  | reward 414.05063 | loss 1131.70240\n",
      "Episode   80 | test  | reward 410.81250 | loss 1133.16592\n",
      "Episode   81 | test  | reward 408.95062 | loss 1134.37277\n",
      "Episode   82 | test  | reward 410.85366 | loss 1132.89141\n",
      "Episode   83 | test  | reward 411.08434 | loss 1132.12900\n",
      "Episode   84 | test  | reward 411.25000 | loss 1130.62278\n",
      "Episode   85 | test  | reward 409.76471 | loss 1131.94050\n",
      "Episode   86 | test  | reward 411.10465 | loss 1128.39266\n",
      "Episode   87 | test  | reward 415.11494 | loss 1128.51314\n",
      "Episode   88 | test  | reward 413.69318 | loss 1127.83647\n",
      "Episode   89 | test  | reward 412.02247 | loss 1123.57866\n",
      "Episode   90 | test  | reward 410.61111 | loss 1124.64388\n",
      "Episode   91 | test  | reward 409.45055 | loss 1127.69160\n",
      "Episode   92 | test  | reward 408.96739 | loss 1126.96247\n",
      "Episode   93 | test  | reward 409.30108 | loss 1124.47656\n",
      "Episode   94 | test  | reward 410.58511 | loss 1132.20578\n",
      "Episode   95 | test  | reward 412.52632 | loss 1136.22504\n",
      "Episode   96 | test  | reward 412.29167 | loss 1134.04247\n",
      "Episode   97 | test  | reward 410.97938 | loss 1133.41778\n",
      "Episode   98 | test  | reward 409.69388 | loss 1136.18533\n",
      "Episode   99 | test  | reward 407.97980 | loss 1135.59962\n",
      "Episode  100 | test  | reward 407.90000 | loss 1136.67153\n",
      "Episode  101 | test  | reward 407.50000 | loss 1135.98354\n",
      "Episode  102 | test  | reward 409.40000 | loss 1135.95349\n",
      "Episode  103 | test  | reward 408.10000 | loss 1136.73610\n",
      "Episode  104 | test  | reward 407.15000 | loss 1134.45450\n",
      "Episode  105 | test  | reward 407.45000 | loss 1134.62390\n",
      "Episode  106 | test  | reward 406.85000 | loss 1136.76275\n",
      "Episode  107 | test  | reward 405.80000 | loss 1134.25958\n",
      "Episode  108 | test  | reward 405.25000 | loss 1128.03772\n",
      "Episode  109 | test  | reward 402.65000 | loss 1127.94428\n",
      "Episode  110 | test  | reward 403.95000 | loss 1126.36811\n",
      "Episode  111 | test  | reward 403.70000 | loss 1123.06955\n",
      "Episode  112 | test  | reward 403.85000 | loss 1120.57551\n",
      "Episode  113 | test  | reward 398.95000 | loss 1123.58219\n",
      "Episode  114 | test  | reward 398.50000 | loss 1126.16858\n",
      "Episode  115 | test  | reward 399.65000 | loss 1127.18957\n",
      "Episode  116 | test  | reward 398.85000 | loss 1125.27238\n",
      "Episode  117 | test  | reward 402.95000 | loss 1123.36965\n",
      "Episode  118 | test  | reward 398.55000 | loss 1125.24724\n",
      "Episode  119 | test  | reward 399.95000 | loss 1129.60716\n",
      "Episode  120 | test  | reward 396.55000 | loss 1129.54231\n",
      "Episode  121 | test  | reward 397.25000 | loss 1128.91803\n",
      "Episode  122 | test  | reward 398.00000 | loss 1134.33375\n",
      "Episode  123 | test  | reward 395.90000 | loss 1128.82633\n",
      "Episode  124 | test  | reward 392.50000 | loss 1129.46435\n",
      "Episode  125 | test  | reward 392.40000 | loss 1132.10158\n",
      "Episode  126 | test  | reward 393.50000 | loss 1132.96497\n",
      "Episode  127 | test  | reward 394.40000 | loss 1134.75963\n",
      "Episode  128 | test  | reward 393.85000 | loss 1131.92394\n",
      "Episode  129 | test  | reward 388.80000 | loss 1132.97000\n",
      "Episode  130 | test  | reward 390.00000 | loss 1136.34191\n",
      "Episode  131 | test  | reward 388.05000 | loss 1135.23376\n",
      "Episode  132 | test  | reward 387.85000 | loss 1131.74649\n",
      "Episode  133 | test  | reward 390.25000 | loss 1128.63189\n",
      "Episode  134 | test  | reward 391.45000 | loss 1126.06173\n",
      "Episode  135 | test  | reward 392.15000 | loss 1122.43338\n",
      "Episode  136 | test  | reward 394.90000 | loss 1121.21018\n",
      "Episode  137 | test  | reward 394.35000 | loss 1123.15978\n",
      "Episode  138 | test  | reward 399.70000 | loss 1125.27949\n",
      "Episode  139 | test  | reward 401.50000 | loss 1124.96379\n",
      "Episode  140 | test  | reward 402.90000 | loss 1127.77112\n",
      "Episode  141 | test  | reward 403.55000 | loss 1127.49765\n",
      "Episode  142 | test  | reward 403.00000 | loss 1130.26604\n",
      "Episode  143 | test  | reward 402.90000 | loss 1129.75755\n",
      "Episode  144 | test  | reward 403.70000 | loss 1128.63077\n",
      "Episode  145 | test  | reward 400.00000 | loss 1129.06194\n",
      "Episode  146 | test  | reward 400.20000 | loss 1127.36099\n",
      "Episode  147 | test  | reward 398.85000 | loss 1129.13587\n",
      "Episode  148 | test  | reward 399.35000 | loss 1132.95877\n",
      "Episode  149 | test  | reward 397.40000 | loss 1126.40379\n",
      "Episode  150 | test  | reward 394.60000 | loss 1125.39813\n",
      "Episode  151 | test  | reward 390.20000 | loss 1125.77419\n",
      "Episode  152 | test  | reward 392.05000 | loss 1126.03525\n",
      "Episode  153 | test  | reward 389.30000 | loss 1127.44997\n",
      "Episode  154 | test  | reward 386.95000 | loss 1128.10714\n",
      "Episode  155 | test  | reward 384.70000 | loss 1125.33691\n",
      "Episode  156 | test  | reward 386.30000 | loss 1123.82937\n",
      "Episode  157 | test  | reward 384.20000 | loss 1125.11842\n",
      "Episode  158 | test  | reward 384.45000 | loss 1127.18654\n",
      "Episode  159 | test  | reward 386.20000 | loss 1125.65320\n",
      "Episode  160 | test  | reward 390.15000 | loss 1127.25681\n",
      "Episode  161 | test  | reward 389.35000 | loss 1129.97680\n",
      "Episode  162 | test  | reward 395.15000 | loss 1129.32428\n",
      "Episode  163 | test  | reward 394.05000 | loss 1130.00308\n",
      "Episode  164 | test  | reward 391.10000 | loss 1126.92695\n",
      "Episode  165 | test  | reward 388.25000 | loss 1127.85572\n",
      "Episode  166 | test  | reward 390.30000 | loss 1126.22578\n",
      "Episode  167 | test  | reward 390.00000 | loss 1123.30622\n",
      "Episode  168 | test  | reward 387.85000 | loss 1119.86098\n",
      "Episode  169 | test  | reward 388.15000 | loss 1119.09169\n",
      "Episode  170 | test  | reward 388.70000 | loss 1114.92809\n",
      "Episode  171 | test  | reward 389.15000 | loss 1114.54462\n",
      "Episode  172 | test  | reward 386.30000 | loss 1115.96994\n",
      "Episode  173 | test  | reward 387.10000 | loss 1117.50904\n",
      "Episode  174 | test  | reward 388.65000 | loss 1121.89945\n",
      "Episode  175 | test  | reward 390.30000 | loss 1119.10134\n",
      "Episode  176 | test  | reward 386.35000 | loss 1118.65077\n",
      "Episode  177 | test  | reward 386.30000 | loss 1117.58452\n",
      "Episode  178 | test  | reward 387.45000 | loss 1116.85576\n",
      "Episode  179 | test  | reward 383.25000 | loss 1118.34648\n",
      "Episode  180 | test  | reward 386.40000 | loss 1116.52705\n",
      "Episode  181 | test  | reward 388.50000 | loss 1116.09037\n",
      "Episode  182 | test  | reward 387.40000 | loss 1116.10561\n",
      "Episode  183 | test  | reward 388.00000 | loss 1114.09867\n",
      "Episode  184 | test  | reward 386.65000 | loss 1113.78348\n",
      "Episode  185 | test  | reward 387.25000 | loss 1114.76579\n",
      "Episode  186 | test  | reward 384.10000 | loss 1118.01755\n",
      "Episode  187 | test  | reward 381.35000 | loss 1116.64333\n",
      "Episode  188 | test  | reward 382.15000 | loss 1115.76759\n",
      "Episode  189 | test  | reward 385.55000 | loss 1117.97637\n",
      "Episode  190 | test  | reward 388.65000 | loss 1115.64970\n",
      "Episode  191 | test  | reward 387.45000 | loss 1112.64042\n",
      "Episode  192 | test  | reward 386.40000 | loss 1114.68732\n",
      "Episode  193 | test  | reward 387.45000 | loss 1121.74572\n",
      "Episode  194 | test  | reward 383.10000 | loss 1116.87236\n",
      "Episode  195 | test  | reward 380.60000 | loss 1111.22993\n",
      "Episode  196 | test  | reward 380.55000 | loss 1113.84025\n",
      "Episode  197 | test  | reward 381.00000 | loss 1114.57985\n",
      "Episode  198 | test  | reward 381.35000 | loss 1118.49217\n",
      "Episode  199 | test  | reward 384.90000 | loss 1118.74860\n",
      "Episode  200 | test  | reward 383.50000 | loss 1116.66385\n",
      "Episode  201 | test  | reward 382.90000 | loss 1115.38040\n",
      "Episode  202 | test  | reward 381.00000 | loss 1116.03327\n",
      "Episode  203 | test  | reward 380.70000 | loss 1114.92163\n",
      "Episode  204 | test  | reward 385.30000 | loss 1116.67296\n",
      "Episode  205 | test  | reward 381.15000 | loss 1118.83420\n",
      "Episode  206 | test  | reward 380.40000 | loss 1119.12080\n",
      "Episode  207 | test  | reward 380.45000 | loss 1122.21768\n",
      "Episode  208 | test  | reward 381.55000 | loss 1124.94943\n",
      "Episode  209 | test  | reward 381.25000 | loss 1126.54622\n",
      "Episode  210 | test  | reward 378.60000 | loss 1128.79349\n",
      "Episode  211 | test  | reward 382.25000 | loss 1129.62979\n",
      "Episode  212 | test  | reward 384.45000 | loss 1131.00052\n",
      "Episode  213 | test  | reward 383.90000 | loss 1129.68316\n",
      "Episode  214 | test  | reward 384.95000 | loss 1129.73515\n",
      "Episode  215 | test  | reward 383.30000 | loss 1129.63562\n",
      "Episode  216 | test  | reward 382.15000 | loss 1129.35587\n",
      "Episode  217 | test  | reward 378.50000 | loss 1130.06563\n",
      "Episode  218 | test  | reward 379.80000 | loss 1128.28559\n",
      "Episode  219 | test  | reward 382.55000 | loss 1123.75853\n",
      "Episode  220 | test  | reward 384.55000 | loss 1123.93495\n",
      "Episode  221 | test  | reward 386.55000 | loss 1123.22555\n",
      "Episode  222 | test  | reward 388.55000 | loss 1122.28990\n",
      "Episode  223 | test  | reward 387.80000 | loss 1122.27235\n",
      "Episode  224 | test  | reward 387.95000 | loss 1124.25071\n",
      "Episode  225 | test  | reward 389.00000 | loss 1122.09486\n",
      "Episode  226 | test  | reward 390.65000 | loss 1122.84712\n",
      "Episode  227 | test  | reward 388.70000 | loss 1122.51178\n",
      "Episode  228 | test  | reward 390.80000 | loss 1124.51960\n",
      "Episode  229 | test  | reward 390.90000 | loss 1126.10101\n",
      "Episode  230 | test  | reward 390.05000 | loss 1125.87625\n",
      "Episode  231 | test  | reward 390.55000 | loss 1125.03967\n",
      "Episode  232 | test  | reward 392.80000 | loss 1126.87774\n",
      "Episode  233 | test  | reward 390.20000 | loss 1131.04491\n",
      "Episode  234 | test  | reward 387.15000 | loss 1133.00474\n",
      "Episode  235 | test  | reward 387.60000 | loss 1137.89622\n",
      "Episode  236 | test  | reward 384.55000 | loss 1137.77260\n",
      "Episode  237 | test  | reward 385.75000 | loss 1138.53988\n",
      "Episode  238 | test  | reward 380.70000 | loss 1137.36101\n",
      "Episode  239 | test  | reward 380.55000 | loss 1138.21502\n",
      "Episode  240 | test  | reward 377.05000 | loss 1135.87547\n",
      "Episode  241 | test  | reward 375.00000 | loss 1137.22314\n",
      "Episode  242 | test  | reward 379.05000 | loss 1133.98752\n",
      "Episode  243 | test  | reward 379.00000 | loss 1132.63771\n",
      "Episode  244 | test  | reward 381.40000 | loss 1134.31357\n",
      "Episode  245 | test  | reward 384.00000 | loss 1132.60703\n",
      "Episode  246 | test  | reward 383.40000 | loss 1134.92869\n",
      "Episode  247 | test  | reward 391.30000 | loss 1135.39286\n",
      "Episode  248 | test  | reward 391.75000 | loss 1134.01680\n",
      "Episode  249 | test  | reward 391.50000 | loss 1132.06242\n",
      "Episode  250 | test  | reward 390.95000 | loss 1133.24658\n",
      "Episode  251 | test  | reward 391.90000 | loss 1132.24130\n",
      "Episode  252 | test  | reward 389.35000 | loss 1130.94328\n",
      "Episode  253 | test  | reward 393.00000 | loss 1128.72261\n",
      "Episode  254 | test  | reward 393.30000 | loss 1131.40720\n",
      "Episode  255 | test  | reward 392.35000 | loss 1133.12919\n",
      "Episode  256 | test  | reward 392.55000 | loss 1133.24843\n",
      "Episode  257 | test  | reward 392.55000 | loss 1130.67718\n",
      "Episode  258 | test  | reward 393.20000 | loss 1128.20848\n",
      "Episode  259 | test  | reward 389.50000 | loss 1127.20844\n",
      "Episode  260 | test  | reward 385.90000 | loss 1129.85279\n",
      "Episode  261 | test  | reward 385.10000 | loss 1127.67972\n",
      "Episode  262 | test  | reward 379.45000 | loss 1132.64360\n",
      "Episode  263 | test  | reward 379.75000 | loss 1131.61341\n",
      "Episode  264 | test  | reward 382.70000 | loss 1132.63191\n",
      "Episode  265 | test  | reward 383.50000 | loss 1132.33646\n",
      "Episode  266 | test  | reward 384.80000 | loss 1131.55940\n",
      "Episode  267 | test  | reward 388.30000 | loss 1130.00937\n",
      "Episode  268 | test  | reward 386.95000 | loss 1128.10970\n",
      "Episode  269 | test  | reward 387.90000 | loss 1126.82810\n",
      "Episode  270 | test  | reward 388.25000 | loss 1129.05349\n",
      "Episode  271 | test  | reward 389.20000 | loss 1127.83268\n",
      "Episode  272 | test  | reward 390.25000 | loss 1128.40447\n",
      "Episode  273 | test  | reward 388.35000 | loss 1127.78485\n",
      "Episode  274 | test  | reward 388.30000 | loss 1125.66518\n",
      "Episode  275 | test  | reward 389.75000 | loss 1129.71713\n",
      "Episode  276 | test  | reward 389.00000 | loss 1130.23048\n",
      "Episode  277 | test  | reward 391.30000 | loss 1129.95014\n",
      "Episode  278 | test  | reward 389.55000 | loss 1130.28879\n",
      "Episode  279 | test  | reward 388.15000 | loss 1127.55570\n",
      "Episode  280 | test  | reward 385.30000 | loss 1128.83271\n",
      "Episode  281 | test  | reward 383.45000 | loss 1126.45137\n",
      "Episode  282 | test  | reward 382.40000 | loss 1129.21693\n",
      "Episode  283 | test  | reward 381.30000 | loss 1131.67253\n",
      "Episode  284 | test  | reward 381.25000 | loss 1132.86537\n",
      "Episode  285 | test  | reward 381.45000 | loss 1130.23417\n",
      "Episode  286 | test  | reward 385.20000 | loss 1131.21013\n",
      "Episode  287 | test  | reward 384.25000 | loss 1132.81942\n",
      "Episode  288 | test  | reward 383.40000 | loss 1134.48918\n",
      "Episode  289 | test  | reward 383.20000 | loss 1133.93392\n",
      "Episode  290 | test  | reward 380.60000 | loss 1134.74389\n",
      "Episode  291 | test  | reward 383.55000 | loss 1136.11319\n",
      "Episode  292 | test  | reward 383.55000 | loss 1134.93948\n",
      "Episode  293 | test  | reward 383.70000 | loss 1131.54411\n",
      "Episode  294 | test  | reward 385.00000 | loss 1125.35733\n",
      "Episode  295 | test  | reward 385.80000 | loss 1126.70283\n",
      "Episode  296 | test  | reward 385.50000 | loss 1124.10284\n",
      "Episode  297 | test  | reward 386.45000 | loss 1126.04117\n",
      "Episode  298 | test  | reward 385.85000 | loss 1121.14614\n",
      "Episode  299 | test  | reward 382.80000 | loss 1120.72084\n",
      "Episode  300 | test  | reward 385.70000 | loss 1123.96638\n",
      "Episode  301 | test  | reward 386.20000 | loss 1124.45351\n",
      "Episode  302 | test  | reward 386.90000 | loss 1127.73103\n",
      "Episode  303 | test  | reward 387.45000 | loss 1130.88085\n",
      "Episode  304 | test  | reward 385.85000 | loss 1124.46209\n",
      "Episode  305 | test  | reward 386.90000 | loss 1125.00613\n",
      "Episode  306 | test  | reward 385.95000 | loss 1124.87145\n",
      "Episode  307 | test  | reward 388.35000 | loss 1127.18403\n",
      "Episode  308 | test  | reward 388.10000 | loss 1130.59555\n",
      "Episode  309 | test  | reward 387.10000 | loss 1130.52435\n",
      "Episode  310 | test  | reward 387.90000 | loss 1129.71361\n",
      "Episode  311 | test  | reward 383.75000 | loss 1129.16551\n",
      "Episode  312 | test  | reward 382.00000 | loss 1128.42493\n",
      "Episode  313 | test  | reward 388.50000 | loss 1131.42654\n",
      "Episode  314 | test  | reward 388.45000 | loss 1130.12641\n",
      "Episode  315 | test  | reward 389.55000 | loss 1131.83529\n",
      "Episode  316 | test  | reward 390.50000 | loss 1132.03676\n",
      "Episode  317 | test  | reward 389.55000 | loss 1132.72694\n",
      "Episode  318 | test  | reward 388.50000 | loss 1133.55611\n",
      "Episode  319 | test  | reward 388.00000 | loss 1136.92946\n",
      "Episode  320 | test  | reward 385.85000 | loss 1138.65303\n",
      "Episode  321 | test  | reward 383.60000 | loss 1139.45516\n",
      "Episode  322 | test  | reward 383.50000 | loss 1136.38661\n",
      "Episode  323 | test  | reward 389.00000 | loss 1132.75908\n",
      "Episode  324 | test  | reward 389.50000 | loss 1128.53246\n",
      "Episode  325 | test  | reward 388.15000 | loss 1125.45598\n",
      "Episode  326 | test  | reward 387.80000 | loss 1121.83542\n",
      "Episode  327 | test  | reward 388.55000 | loss 1116.80830\n",
      "Episode  328 | test  | reward 391.55000 | loss 1118.94495\n",
      "Episode  329 | test  | reward 390.40000 | loss 1115.85089\n",
      "Episode  330 | test  | reward 392.25000 | loss 1115.25548\n",
      "Episode  331 | test  | reward 394.30000 | loss 1117.17554\n",
      "Episode  332 | test  | reward 392.00000 | loss 1118.52882\n",
      "Episode  333 | test  | reward 396.10000 | loss 1118.37498\n",
      "Episode  334 | test  | reward 397.35000 | loss 1119.26996\n",
      "Episode  335 | test  | reward 397.45000 | loss 1115.85117\n",
      "Episode  336 | test  | reward 397.75000 | loss 1116.01060\n",
      "Episode  337 | test  | reward 395.50000 | loss 1115.04362\n",
      "Episode  338 | test  | reward 398.40000 | loss 1114.67423\n",
      "Episode  339 | test  | reward 394.65000 | loss 1115.48313\n",
      "Episode  340 | test  | reward 396.40000 | loss 1117.73389\n",
      "Episode  341 | test  | reward 397.60000 | loss 1115.95504\n",
      "Episode  342 | test  | reward 393.25000 | loss 1116.69978\n",
      "Episode  343 | test  | reward 393.40000 | loss 1114.18095\n",
      "Episode  344 | test  | reward 388.00000 | loss 1113.79762\n",
      "Episode  345 | test  | reward 386.50000 | loss 1114.40176\n",
      "Episode  346 | test  | reward 386.10000 | loss 1114.89832\n",
      "Episode  347 | test  | reward 379.30000 | loss 1113.67925\n",
      "Episode  348 | test  | reward 378.90000 | loss 1111.72221\n",
      "Episode  349 | test  | reward 380.35000 | loss 1116.98657\n",
      "Episode  350 | test  | reward 381.35000 | loss 1117.16054\n",
      "Episode  351 | test  | reward 385.10000 | loss 1115.78822\n",
      "Episode  352 | test  | reward 385.35000 | loss 1118.42380\n",
      "Episode  353 | test  | reward 388.70000 | loss 1121.08072\n",
      "Episode  354 | test  | reward 388.30000 | loss 1121.46059\n",
      "Episode  355 | test  | reward 387.75000 | loss 1122.22285\n",
      "Episode  356 | test  | reward 385.05000 | loss 1123.27484\n",
      "Episode  357 | test  | reward 385.90000 | loss 1123.95146\n",
      "Episode  358 | test  | reward 386.35000 | loss 1121.73568\n",
      "Episode  359 | test  | reward 386.20000 | loss 1126.70941\n",
      "Episode  360 | test  | reward 385.80000 | loss 1123.28444\n",
      "Episode  361 | test  | reward 388.65000 | loss 1122.78607\n",
      "Episode  362 | test  | reward 389.45000 | loss 1120.92214\n",
      "Episode  363 | test  | reward 389.55000 | loss 1122.53307\n",
      "Episode  364 | test  | reward 386.90000 | loss 1122.35919\n",
      "Episode  365 | test  | reward 385.25000 | loss 1126.60187\n",
      "Episode  366 | test  | reward 381.95000 | loss 1128.40005\n",
      "Episode  367 | test  | reward 380.75000 | loss 1131.68910\n",
      "Episode  368 | test  | reward 382.00000 | loss 1136.40593\n",
      "Episode  369 | test  | reward 381.10000 | loss 1137.39716\n",
      "Episode  370 | test  | reward 381.40000 | loss 1136.65854\n",
      "Episode  371 | test  | reward 382.60000 | loss 1138.36765\n",
      "Episode  372 | test  | reward 385.20000 | loss 1136.24451\n",
      "Episode  373 | test  | reward 388.30000 | loss 1133.21001\n",
      "Episode  374 | test  | reward 388.05000 | loss 1136.92174\n",
      "Episode  375 | test  | reward 386.25000 | loss 1135.19741\n",
      "Episode  376 | test  | reward 389.60000 | loss 1134.39355\n",
      "Episode  377 | test  | reward 387.45000 | loss 1136.92259\n",
      "Episode  378 | test  | reward 390.80000 | loss 1137.44198\n",
      "Episode  379 | test  | reward 394.45000 | loss 1139.87989\n",
      "Episode  380 | test  | reward 396.60000 | loss 1139.37409\n",
      "Episode  381 | test  | reward 396.90000 | loss 1139.38579\n",
      "Episode  382 | test  | reward 398.30000 | loss 1134.50406\n",
      "Episode  383 | test  | reward 401.20000 | loss 1132.23552\n",
      "Episode  384 | test  | reward 403.50000 | loss 1130.67648\n",
      "Episode  385 | test  | reward 401.50000 | loss 1130.62389\n",
      "Episode  386 | test  | reward 397.75000 | loss 1130.49588\n",
      "Episode  387 | test  | reward 399.20000 | loss 1133.87094\n",
      "Episode  388 | test  | reward 400.85000 | loss 1132.99710\n",
      "Episode  389 | test  | reward 398.20000 | loss 1137.16504\n",
      "Episode  390 | test  | reward 396.95000 | loss 1138.72311\n",
      "Episode  391 | test  | reward 394.45000 | loss 1137.96851\n",
      "Episode  392 | test  | reward 395.60000 | loss 1137.10753\n",
      "Episode  393 | test  | reward 395.45000 | loss 1137.05691\n",
      "Episode  394 | test  | reward 401.20000 | loss 1141.30912\n",
      "Episode  395 | test  | reward 400.30000 | loss 1140.56986\n",
      "Episode  396 | test  | reward 401.25000 | loss 1142.30756\n",
      "Episode  397 | test  | reward 402.45000 | loss 1140.87583\n",
      "Episode  398 | test  | reward 403.75000 | loss 1135.67667\n",
      "Episode  399 | test  | reward 403.95000 | loss 1139.77662\n",
      "Episode  400 | test  | reward 404.15000 | loss 1135.94894\n",
      "Episode  401 | test  | reward 404.25000 | loss 1137.42333\n",
      "Episode  402 | test  | reward 404.40000 | loss 1132.95537\n",
      "Episode  403 | test  | reward 404.70000 | loss 1132.02297\n",
      "Episode  404 | test  | reward 405.65000 | loss 1139.02259\n",
      "Episode  405 | test  | reward 408.25000 | loss 1136.58594\n",
      "Episode  406 | test  | reward 410.25000 | loss 1136.28993\n",
      "Episode  407 | test  | reward 408.80000 | loss 1133.35731\n",
      "Episode  408 | test  | reward 410.95000 | loss 1131.60830\n",
      "Episode  409 | test  | reward 410.60000 | loss 1127.20466\n",
      "Episode  410 | test  | reward 410.85000 | loss 1126.26878\n",
      "Episode  411 | test  | reward 410.15000 | loss 1130.86714\n",
      "Episode  412 | test  | reward 409.75000 | loss 1130.51346\n",
      "Episode  413 | test  | reward 405.80000 | loss 1130.64832\n",
      "Episode  414 | test  | reward 405.50000 | loss 1129.03157\n",
      "Episode  415 | test  | reward 407.60000 | loss 1125.46260\n",
      "Episode  416 | test  | reward 405.90000 | loss 1123.77821\n",
      "Episode  417 | test  | reward 405.95000 | loss 1124.99233\n",
      "Episode  418 | test  | reward 407.60000 | loss 1125.29823\n",
      "Episode  419 | test  | reward 404.60000 | loss 1127.34604\n",
      "Episode  420 | test  | reward 407.50000 | loss 1128.16139\n",
      "Episode  421 | test  | reward 407.05000 | loss 1128.49473\n",
      "Episode  422 | test  | reward 405.05000 | loss 1129.16516\n",
      "Episode  423 | test  | reward 399.60000 | loss 1128.17128\n",
      "Episode  424 | test  | reward 396.50000 | loss 1133.27865\n",
      "Episode  425 | test  | reward 397.80000 | loss 1137.62146\n",
      "Episode  426 | test  | reward 397.70000 | loss 1136.87502\n",
      "Episode  427 | test  | reward 397.50000 | loss 1138.46187\n",
      "Episode  428 | test  | reward 392.85000 | loss 1136.68752\n",
      "Episode  429 | test  | reward 395.20000 | loss 1139.86829\n",
      "Episode  430 | test  | reward 393.90000 | loss 1139.83349\n",
      "Episode  431 | test  | reward 392.00000 | loss 1141.44111\n",
      "Episode  432 | test  | reward 391.15000 | loss 1141.21306\n",
      "Episode  433 | test  | reward 386.80000 | loss 1139.54745\n",
      "Episode  434 | test  | reward 390.30000 | loss 1136.98377\n",
      "Episode  435 | test  | reward 392.55000 | loss 1136.93962\n",
      "Episode  436 | test  | reward 395.00000 | loss 1138.02887\n",
      "Episode  437 | test  | reward 394.75000 | loss 1136.11248\n",
      "Episode  438 | test  | reward 392.80000 | loss 1134.11130\n",
      "Episode  439 | test  | reward 394.25000 | loss 1130.87576\n",
      "Episode  440 | test  | reward 392.20000 | loss 1132.36206\n",
      "Episode  441 | test  | reward 391.55000 | loss 1133.77549\n",
      "Episode  442 | test  | reward 391.85000 | loss 1135.33564\n",
      "Episode  443 | test  | reward 393.60000 | loss 1137.83982\n",
      "Episode  444 | test  | reward 394.30000 | loss 1139.12586\n",
      "Episode  445 | test  | reward 392.70000 | loss 1140.88681\n",
      "Episode  446 | test  | reward 393.90000 | loss 1142.56918\n",
      "Episode  447 | test  | reward 396.95000 | loss 1141.21904\n",
      "Episode  448 | test  | reward 398.10000 | loss 1140.41727\n",
      "Episode  449 | test  | reward 396.75000 | loss 1136.84588\n",
      "Episode  450 | test  | reward 396.95000 | loss 1134.77228\n",
      "Episode  451 | test  | reward 393.75000 | loss 1136.95515\n",
      "Episode  452 | test  | reward 396.80000 | loss 1133.81511\n",
      "Episode  453 | test  | reward 390.45000 | loss 1129.05971\n",
      "Episode  454 | test  | reward 389.85000 | loss 1128.53638\n",
      "Episode  455 | test  | reward 391.45000 | loss 1124.43874\n",
      "Episode  456 | test  | reward 393.30000 | loss 1123.21143\n",
      "Episode  457 | test  | reward 392.70000 | loss 1124.56248\n",
      "Episode  458 | test  | reward 390.15000 | loss 1127.38413\n",
      "Episode  459 | test  | reward 389.35000 | loss 1127.37860\n",
      "Episode  460 | test  | reward 390.35000 | loss 1130.91799\n",
      "Episode  461 | test  | reward 386.70000 | loss 1130.19502\n",
      "Episode  462 | test  | reward 386.10000 | loss 1127.83410\n",
      "Episode  463 | test  | reward 388.45000 | loss 1126.42652\n",
      "Episode  464 | test  | reward 386.85000 | loss 1127.86994\n",
      "Episode  465 | test  | reward 386.85000 | loss 1122.58223\n",
      "Episode  466 | test  | reward 389.55000 | loss 1121.26695\n",
      "Episode  467 | test  | reward 389.65000 | loss 1119.62210\n",
      "Episode  468 | test  | reward 392.85000 | loss 1117.09858\n",
      "Episode  469 | test  | reward 395.05000 | loss 1115.56066\n",
      "Episode  470 | test  | reward 396.70000 | loss 1116.27771\n",
      "Episode  471 | test  | reward 393.30000 | loss 1117.77399\n",
      "Episode  472 | test  | reward 393.10000 | loss 1115.84666\n",
      "Episode  473 | test  | reward 390.20000 | loss 1123.54962\n",
      "Episode  474 | test  | reward 391.05000 | loss 1120.61377\n",
      "Episode  475 | test  | reward 391.40000 | loss 1119.91840\n",
      "Episode  476 | test  | reward 386.90000 | loss 1121.09710\n",
      "Episode  477 | test  | reward 387.95000 | loss 1117.72456\n",
      "Episode  478 | test  | reward 384.60000 | loss 1119.41669\n",
      "Episode  479 | test  | reward 383.90000 | loss 1119.33353\n",
      "Episode  480 | test  | reward 383.50000 | loss 1120.91565\n",
      "Episode  481 | test  | reward 382.70000 | loss 1122.93047\n",
      "Episode  482 | test  | reward 380.40000 | loss 1124.77281\n",
      "Episode  483 | test  | reward 375.55000 | loss 1127.75622\n",
      "Episode  484 | test  | reward 373.75000 | loss 1130.37492\n",
      "Episode  485 | test  | reward 380.80000 | loss 1129.35063\n",
      "Episode  486 | test  | reward 382.90000 | loss 1128.59588\n",
      "Episode  487 | test  | reward 379.65000 | loss 1122.59850\n",
      "Episode  488 | test  | reward 378.45000 | loss 1126.05938\n",
      "Episode  489 | test  | reward 378.40000 | loss 1125.28384\n",
      "Episode  490 | test  | reward 381.75000 | loss 1123.76778\n",
      "Episode  491 | test  | reward 382.30000 | loss 1120.11497\n",
      "Episode  492 | test  | reward 381.85000 | loss 1118.11198\n",
      "Episode  493 | test  | reward 378.65000 | loss 1113.93051\n"
     ]
    }
   ],
   "source": [
    "metrics = None\n",
    "if testing_mode:\n",
    "    # TODO: Load your saved online_dqn model for evaluation\n",
    "    # ...\n",
    "    loaded_pth_file = torch.load(test_model_directory)\n",
    "    online_dqn = DeepQNet(h,w,image_stack,num_actions)\n",
    "    online_dqn.load_state_dict(loaded_pth_file['online_dqn'])\n",
    "    online_dqn.cuda()\n",
    "    test_metrics = dict(reward=[], loss=[])\n",
    "    for it in range(max_test_episodes):\n",
    "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=False)\n",
    "        update_metrics(test_metrics, episode_metrics)\n",
    "        print_metrics(it + 1, test_metrics, is_training=False)\n",
    "        save_m(test_metrics)\n",
    "\n",
    "    metrics = test_metrics\n",
    "else:\n",
    "    print(\"Training\")\n",
    "    train_metrics = dict(reward=[], loss=[])\n",
    "    for it in range(max_train_episodes):\n",
    "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=True)\n",
    "        update_metrics(train_metrics, episode_metrics)\n",
    "        if curr_step > burn_in_phase and eps > min_eps:\n",
    "            eps *= eps_decay\n",
    "        if it % 50 == 0:\n",
    "            print_metrics(it, train_metrics, is_training=True)\n",
    "            save_checkpoint(curr_step, eps, train_metrics)\n",
    "\n",
    "        # print(f\"episode: {it} done!\")\n",
    "        save_m(train_metrics)\n",
    "    metrics = train_metrics\n",
    "    # save_m(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQbMdor_dCnp"
   },
   "outputs": [],
   "source": [
    "  plot_metrics(metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
