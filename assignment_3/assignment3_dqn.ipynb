{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4538,
     "status": "ok",
     "timestamp": 1673444557316,
     "user": {
      "displayName": "Mohammad Milad Tufan",
      "userId": "07157501036840722784"
     },
     "user_tz": -60
    },
    "id": "rr1d-kHohxGL",
    "outputId": "dc26b8c4-79da-4e55-a18a-c5810b3917da",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gym[accept-rom-license,atari]==0.25.2 in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (6.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (0.7.5)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.25.2) (0.4.2)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]==0.25.2) (5.10.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2.25.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (0.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]==0.25.2) (3.11.0)\n",
      "Requirement already satisfied: libtorrent in /usr/local/lib/python3.8/dist-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.25.2) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari,accept-rom-license]==0.25.2\n",
    "import sys, os\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1673444557819,
     "user": {
      "displayName": "Mohammad Milad Tufan",
      "userId": "07157501036840722784"
     },
     "user_tz": -60
    },
    "id": "IGCa_JQeiy1F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "from gym.spaces import Box\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        transform = torchvision.transforms.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(self.shape),\n",
    "                                                     torchvision.transforms.Normalize(0, 255)])\n",
    "        return transforms(observation).squeeze(0)\n",
    "\n",
    "\n",
    "class ExperienceReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def store(self, state, next_state, action, reward, done):\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "        self.memory.append((state, next_state, action, reward, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # TODO: uniformly sample batches of Tensors for: state, next_state, action, reward, done\n",
    "        # ...\n",
    "\n",
    "\n",
    "        # uniformly get batch with batch_size\n",
    "        sampled_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "\n",
    "        # save to arrays\n",
    "        for (curr_state, next_state, action, reward, done) in sampled_batch:\n",
    "            states.append(curr_state)\n",
    "            next_states.append(next_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "\n",
    "        return  torch.tensor(np.array(states)), \\\n",
    "                torch.tensor(np.array(next_states)), \\\n",
    "                torch.tensor(np.array(actions)), \\\n",
    "                torch.tensor(np.array(rewards)), \\\n",
    "                torch.tensor(np.array(dones))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1673445852046,
     "user": {
      "displayName": "Mohammad Milad Tufan",
      "userId": "07157501036840722784"
     },
     "user_tz": -60
    },
    "id": "LOCiUgfBjJYc",
    "outputId": "d5c70485-64be-4136-920a-51a92ddecdb8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stacked frames:  4\n",
      "Resized observation space dimensionality:  84 84\n",
      "Number of available actions by the agent:  6\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "from gym.wrappers import FrameStack\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "env_rendering = False    # Set to False while training your model on Colab\n",
    "testing_mode = False\n",
    "test_model_directory = './your_saved_model.pth.tar'\n",
    "\n",
    "# Create and preprocess the Space Invaders environment\n",
    "if env_rendering:\n",
    "    env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space=False, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space=False)\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "image_stack, h, w = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "print('Number of stacked frames: ', image_stack)\n",
    "print('Resized observation space dimensionality: ', h, w)\n",
    "print('Number of available actions by the agent: ', num_actions)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 61\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.backends.cudnn.enabled:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Hyperparameters (to be modified)\n",
    "batch_size = 32\n",
    "alpha = 0.00025\n",
    "gamma = 0.95\n",
    "eps, eps_decay, min_eps = 1.0, 0.999, 0.05\n",
    "buffer = ExperienceReplayMemory(20000)\n",
    "burn_in_phase = 20000\n",
    "sync_target = 30000\n",
    "max_train_frames = 10000\n",
    "max_train_episodes = 1000# 100000\n",
    "max_test_episodes = 1\n",
    "curr_step = 0\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2049,
     "status": "ok",
     "timestamp": 1673444560289,
     "user": {
      "displayName": "Mohammad Milad Tufan",
      "userId": "07157501036840722784"
     },
     "user_tz": -60
    },
    "id": "NNoUgNjujqRX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert(x):\n",
    "    return torch.tensor(x.__array__()).float()\n",
    "\n",
    "\n",
    "class DeepQNet(torch.nn.Module):\n",
    "    def __init__(self, h, w, image_stack, num_actions):\n",
    "        super(DeepQNet, self).__init__()\n",
    "        # TODO: create a convolutional neural network\n",
    "        # ...\n",
    "\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4, 6, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Conv2d(6,16,5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        self.out_size = self.get_out(h,w)\n",
    "        self.fully_connected_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.out_size, 128),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.Linear(64, num_actions)\n",
    "        )\n",
    "\n",
    "    def get_out(self, h,w):\n",
    "        out = self.conv(torch.zeros(1, 4, h, w))\n",
    "        return int(np.prod(out.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: forward pass from the neural network\n",
    "        # ...\n",
    "        x_new = self.conv(x)\n",
    "        x_new = x_new.view(-1, self.out_size)\n",
    "        return self.fully_connected_layers(x_new)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: create an online and target DQN (Hint: Use copy.deepcopy() and requires_grad utilities!)\n",
    "# ...\n",
    "online_dqn = DeepQNet(h,w,image_stack, num_actions)\n",
    "target_dqn = copy.deepcopy(online_dqn)\n",
    "online_dqn.to(device)\n",
    "target_dqn.to(device)\n",
    "\n",
    "\n",
    "# TODO: create the appropriate MSE criterion and Adam optimizer\n",
    "# ...\n",
    "optimizer = torch.optim.Adam(online_dqn.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1673445402290,
     "user": {
      "displayName": "Mohammad Milad Tufan",
      "userId": "07157501036840722784"
     },
     "user_tz": -60
    },
    "id": "RO21LQJ6j0WC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def policy(state, is_training):\n",
    "    global eps\n",
    "    state = convert(state).unsqueeze(0).to(device)\n",
    "\n",
    "    #TODO: Implement an epsilon-greedy policy\n",
    "    #...\n",
    "    with torch.no_grad():\n",
    "        if is_training:\n",
    "            p = online_dqn(state)\n",
    "            if np.random.rand() < eps:\n",
    "                a = random_action()\n",
    "            else:\n",
    "                a = torch.argmax(p).tolist()\n",
    "\n",
    "        else:\n",
    "            p = online_dqn(state)\n",
    "            a = torch.argmax(p).tolist()\n",
    "\n",
    "    return a\n",
    "\n",
    "def random_action():\n",
    "    return np.random.randint(0, num_actions)\n",
    "\n",
    "def compute_loss(state, action, reward, next_state, done):\n",
    "    state = convert(state).to(device)\n",
    "    next_state = convert(next_state).to(device)\n",
    "    action = action.to(device)\n",
    "    reward = reward.to(device)\n",
    "    done = done.to(device)\n",
    "\n",
    "    # TODO: Compute the DQN (or DDQN) loss based on the criterion\n",
    "    # ...\n",
    "\n",
    "    # mse loss\n",
    "\n",
    "    target = target_dqn(next_state)\n",
    "    pred = online_dqn(state)\n",
    "    \n",
    "    pred_next_state = online_dqn(next_state)\n",
    "    counter = 0\n",
    "    diff_squared = 0\n",
    "    for st,at,rt,news_t,dt in zip(state, action, reward, next_state, done):\n",
    "        action_max_q = torch.argmax(pred_next_state[counter])\n",
    "\n",
    "        y_target = rt + gamma * target[counter][action_max_q]\n",
    "        diff_squared += (y_target - pred[counter])**2\n",
    "    # huberloss\n",
    "    # loss = torch.nn.SmoothL1Loss().to(device)\n",
    "    loss = torch.sum(diff_squared) / torch.flatten(pred).size()[0]\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_episode(curr_step, buffer, is_training):\n",
    "    global eps\n",
    "    global target_dqn\n",
    "    episode_reward, episode_loss = 0, 0.\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(max_train_frames):\n",
    "        action = policy(state, is_training)\n",
    "        curr_step += 1\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if is_training:\n",
    "            buffer.store(state, next_state, action, reward, done)\n",
    "\n",
    "            if curr_step > burn_in_phase:\n",
    "                state_batch, next_state_batch, action_batch, reward_batch, done_batch = buffer.sample(batch_size)\n",
    "\n",
    "                if curr_step % sync_target == 0:\n",
    "                    # TODO: Periodically update your target_dqn at each sync_target frames\n",
    "                    # ...\n",
    "                    target_dqn = copy.deepcopy(online_dqn)\n",
    "\n",
    "                loss = compute_loss(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                episode_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                episode_loss += compute_loss(state, action, reward, next_state, done).item()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "\n",
    "    return dict(reward=episode_reward, loss=episode_loss / t), curr_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1673444563249,
     "user": {
      "displayName": "Mohammad Milad Tufan",
      "userId": "07157501036840722784"
     },
     "user_tz": -60
    },
    "id": "PEGSGYsQj8Wh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_metrics(metrics, episode):\n",
    "    for k, v in episode.items():\n",
    "        metrics[k].append(v)\n",
    "\n",
    "\n",
    "def print_metrics(it, metrics, is_training, window=100):\n",
    "    reward_mean = np.mean(metrics['reward'][-window:])\n",
    "    loss_mean = np.mean(metrics['loss'][-window:])\n",
    "    mode = \"train\" if is_training else \"test\"\n",
    "    print(f\"Episode {it:4d} | {mode:5s} | reward {reward_mean:5.5f} | loss {loss_mean:5.5f}\")\n",
    "\n",
    "\n",
    "def save_checkpoint(curr_step, eps, train_metrics):\n",
    "    save_dict = {'curr_step': curr_step, \n",
    "                 'train_metrics': train_metrics, \n",
    "                 'eps': eps,\n",
    "                 'online_dqn': online_dqn.state_dict(), \n",
    "                 'target_dqn': target_dqn.state_dict()}\n",
    "    torch.save(save_dict, test_model_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1673444563249,
     "user": {
      "displayName": "Mohammad Milad Tufan",
      "userId": "07157501036840722784"
     },
     "user_tz": -60
    },
    "id": "Uy7C4qfWkzXb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Plot your train_metrics and test_metrics\n",
    "# ...\n",
    "def plot_metrics(metrics, window=100):\n",
    "    reward = metrics['reward'][-window:]\n",
    "    loss = metrics['loss'][-window:]\n",
    "    epsiodes = np.arange(0, max_train_episodes, 1)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(epsiodes, reward)\n",
    "    ax2.plot(epsiodes, loss)\n",
    "\n",
    "    ax1.set_xlabel(\"episodes\")\n",
    "    ax2.set_xlabel(\"episodes\")\n",
    "    ax1.set_ylabel(\"reward\")\n",
    "    ax2.set_ylabel(\"loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3108950,
     "status": "error",
     "timestamp": 1673448964044,
     "user": {
      "displayName": "Mohammad Milad Tufan",
      "userId": "07157501036840722784"
     },
     "user_tz": -60
    },
    "id": "TdhWcoW-kyuF",
    "outputId": "3d25cd92-2d30-455b-e692-20ff89f936fc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0 | train | reward 30.00000 | loss 0.00000\n",
      "Episode   50 | train | reward 196.27451 | loss 0.00000\n",
      "Episode  100 | train | reward 174.20000 | loss 0.00000\n",
      "Episode  150 | train | reward 153.70000 | loss 5.07334\n",
      "Episode  200 | train | reward 160.20000 | loss 29.66748\n",
      "Episode  250 | train | reward 144.90000 | loss 47.61677\n",
      "Episode  300 | train | reward 131.90000 | loss 43.37934\n",
      "Episode  350 | train | reward 133.80000 | loss 42.76970\n",
      "Episode  400 | train | reward 131.10000 | loss 42.74873\n",
      "Episode  450 | train | reward 139.80000 | loss 42.59283\n",
      "Episode  500 | train | reward 135.40000 | loss 42.97650\n",
      "Episode  550 | train | reward 114.55000 | loss 42.00377\n",
      "Episode  600 | train | reward 108.65000 | loss 43.05841\n",
      "Episode  650 | train | reward 109.40000 | loss 37.19815\n",
      "Episode  700 | train | reward 108.55000 | loss 29.88591\n",
      "Episode  750 | train | reward 96.85000 | loss 27.05470\n",
      "Episode  800 | train | reward 92.65000 | loss 24.31295\n",
      "Episode  850 | train | reward 110.75000 | loss 24.89630\n",
      "Episode  900 | train | reward 107.55000 | loss 28.84334\n",
      "Episode  950 | train | reward 100.45000 | loss 31.62560\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-023912578d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# print(f\"episode: {it} done!\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0a355ffe929b>\u001b[0m in \u001b[0;36mplot_metrics\u001b[0;34m(metrics, window)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Horizontally stacked subplots'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsiodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsiodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (100,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if testing_mode:\n",
    "    # TODO: Load your saved online_dqn model for evaluation\n",
    "    # ...\n",
    "    loaded_pth_file = torch.load(test_model_directory)\n",
    "    online_dqn = DeepQNet(h,w,image_stack,num_actions)\n",
    "    online_dqn.load_state_dict(loaded_pth_file['online_dqn'])\n",
    "\n",
    "    test_metrics = dict(reward=[], loss=[])\n",
    "    for it in range(max_test_episodes):\n",
    "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=False)\n",
    "        update_metrics(test_metrics, episode_metrics)\n",
    "        print_metrics(it + 1, test_metrics, is_training=False)\n",
    "    plot_metrics(test_metrics)\n",
    "else:\n",
    "    train_metrics = dict(reward=[], loss=[])\n",
    "    for it in range(max_train_episodes):\n",
    "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=True)\n",
    "        update_metrics(train_metrics, episode_metrics)\n",
    "        if curr_step > burn_in_phase and eps > min_eps:\n",
    "            eps *= eps_decay\n",
    "        if it % 50 == 0:\n",
    "            print_metrics(it, train_metrics, is_training=True)\n",
    "            save_checkpoint(curr_step, eps, train_metrics)\n",
    "        # print(f\"episode: {it} done!\")\n",
    "    plot_metrics(train_metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
